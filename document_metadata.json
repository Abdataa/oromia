{"1": "goodness of \ufb01t\nBy the end of this section, you\u2019ll be able to\n\u2022 de\ufb01ne (in both math and code) a class of\nlinear hypotheses for given featurized data\n\u2022 compute the perceptron and hinge losses a\ngiven hypothesis suffers on given data\nwhat hypotheses will we consider? a recipe for H \u2014 Remember: we want our machine\nto \ufb01nd an input-to-output rule. We call such rules hypotheses. As engineers,\nwe carve out a menu H of rules for the machine to choose from. We\u2019ll consider\nhypotheses of this format: extract features of the input to make a list of numbers;\nthen linearly combine those numbers to make another list of numbers; \ufb01nally, read\nout a prediction from the latter list. Our digit classifying hypotheses, for instance,\nlook like:\u25e6\n\u2190Our list threeness has length one: it\u2019s just\na fancy way of talking about a single number.\nWe\u2019ll later use longer lists to model richer out-\nputs: to classify between > 2 labels, to generate\na whole image instead of a class label, etc. The\nconcept of threeness (and what plays the same\nrole in other ML projects) is called the decision\nfunction. The decision function takes an input\nx and returns some kind of score for what the\n\ufb01nal output should be. The decision function\nvalue is what we feed into the readout.\ndef predict(x):\nfeatures = [brightness(x), width(x)]\n# featurize\nthreeness = [ -1.*features[0] +4.*features[1] ]\n# linearly combine\nprediction = 3 if threeness[0]>0. else 1\n# read out prediction\nreturn prediction\nThe various hypotheses differ only in those coef\ufb01cients (jargon: weights, here\n\u22121, +4) for their linear combinations; it is these degrees of freedom that the\nmachine learns from data. These arrows summarize the situation:\u25e6\n\u2190This triple of arrows is standard in ML. But\nthe name \u2018readout\u2019 is not standard jargon.\nI\ndon\u2019t know standard jargon for this arrow.\nX\nfeaturize\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2192\nnot learned R2 linearly combine\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2192\nlearned!\nR1\nread out\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2192\nnot learned Y\nOur Unit 1 motto is to learn a linear map \ufb02anked by hand-coded nonlinearities. We\ndesign the nonlinearities to capture domain knowledge about our data and goals.\nFigure 7: The slope of the decision function\u2019s\ngraph is meaningful, even though it doesn\u2019t\naffect the decision boundary. We plot the de-\ncision function (vertical axis) over weight-space\n(horizontal axes) (black dot). The graph is an\nin\ufb01nite plane, a hexagonal portion of which we\nshow; it attains zero altitude at the decision\nboundary. We interpret high (low) altitude as\n\u2018con\ufb01dent the label is orange (blue)\u2019; interme-\ndiate altitudes, as leaning but without con\ufb01-\ndence. We\u2019ll introduce a notion of \u2018goodness-\nof-\ufb01t-to-data\u2019 that seeks not just to correctly\nclassify all the training points BUT ALSO to do\nso with con\ufb01dence. That is, we want orange\n(blue training points to all be above (below)\nthe gray \u2018shelf\u2019 that delimits high-con\ufb01dence.\nI like to imagine a shoreline: an interface be-\ntween water and beach. We want to adjust the\ntopography so that all blue (orange) training\npoints are deep underwater (high and dry).\nFor now, we\u2019ll assume we\u2019ve already decided on our featurization and we\u2019ll\nuse the same readout as in the code above:\u25e6\n\u2190Intuitively, threeness is the machine\u2019s con\ufb01-\ndence that the answer is 3. Our current read-\nout discards all information except for the sign:\nwe forget whether the machine was very con-\n\ufb01dent or slightly con\ufb01dent that the answer is\n3, so long as it prefers 3 over 1. However, we\nwill care about con\ufb01dence levels when assess-\ning goodness-of-\ufb01t.\nprediction = 3 if threeness[0]>0. else 1\n# for a digit-classifying project\nprediction = \u2019cow\u2019 if bovinity[0]>0. else \u2019dog\u2019\n# for an animal-classifying project\nThat\u2019s how we make predictions from those \u201cdecision function values\u201d threeness\nor bovinity. But to compute those values we need to determine some weights.\nHow shall we determine our weights and hence our hypothesis?\nhow good is a hypothesis? fit \u2014 We instruct our machine to \ufb01nd within our menu H\na hypothesis that\u2019s as \u201cgood\u201d as possible. That is, the hypothesis should both\n\ufb01t our training data and seem intrinsically plausible. We want to quantify these\nnotions of goodness-of-\ufb01t and intrinsic-plausibility. As with H, how we quantify\nthese notions is an engineering art informed by domain knowledge. Still, there\nare patterns and principles \u2014 we will study two speci\ufb01c quantitative notions,\nthe perceptron loss and SVM loss, to study these principles. Later, once we un-\nderstand these notions as quantifying uncertainty (i.e., as probabilistic notions),\nwe\u2019ll appreciate their logic. But for now we\u2019ll bravely venture forth, ad hoc!\nWe start with goodness-of-\ufb01t. Hypotheses correspond\u25e6to weights. For exam-\n\u2190A very careful reader might ask: can\u2019t mul-\ntiple choices of weights determine the same hy-\npothesis? E.g. (\u22121, +4) and (\u221210, +40) classify\nevery input the same way, since they either\nboth make threeness positive or both make\nthreeness negative. This is a very good point,\ndear reader, but at this stage in the course,\nmuch too pedantic! Ask again later.\nple, the weight vector (\u22121, +4) determines the hypothesis listed above.\nOne way to quantify h\u2019s goodness-of-\ufb01t to a training example (x, y) is to\nsee whether or not h correctly predicts y from x. That is, we could quantify\ngoodness-of-\ufb01t by training accuracy, like we did in the previous digits example:\ndef is_correct(x,y,a,b):\nthreeness = a*brightness(x) + b*width(x)\nprediction = 3 if threeness>0. else 1\nreturn 1. if prediction==y else 0.\nBy historical convention we actually like\u25e6to minimize badness (jargon: loss)\n\u2190ML can be a glass half-empty kind of subject!\nrather than maximize goodness. So we\u2019ll rewrite the above in terms of mistakes:\u25e6\n\u2190We\u2019ll refer to the \u201cleeway before a mistake\u201d\nthroughout this section; it\u2019s a standard concept\nbut not a concept with a standard name afaik.\ndef leeway_before_mistake(x,y,a,b):\nthreeness = a*brightness(x) + b*width(x)\nreturn +threeness if y==3 else -threeness\ndef is_mistake(x,y,a,b):\nreturn 0. leeway_before_mistake(x,y,a,b)>0. else 1.\nWe could de\ufb01ne goodness-of-\ufb01t as training accuracy. But we\u2019ll enjoy better\ngeneralization and easier optimization by allowing \u201cpartial credit\u201d for borderline\npredictions. E.g. we could use leeway_before_mistake as goodness-of-\ufb01t:\u25e6\n\u2190to de\ufb01ne loss, we \ufb02ip signs, hence the \u20181\u2212\u2019\nFood For Thought: For incentives to point the\nright way, loss should decrease as threeness\nincreases when\ny==3 but should increase as\nthreeness increases when y==1. Verify these re-\nlations for the several loss functions we de\ufb01ne.\ndef linear_loss(x,y,a,b):\nreturn 1 - leeway_before_mistake(x,y,a,b)\nFigure 8: Geometry of leeways, margins, and\nhinge loss. As before, we graph the decision\nfunction (vertical axis) against weight-space.\nThe y = \u22121 (y = +1) training points (x, y) sit\none unit below (above) the zero-altitude plane.\nWe draw regions where the graph surpasses\n\u00b11 altitude as shaded \u2018shelves\u2019.\nLeeway is\nthe vertical distance between a gray x and the\ngraph (and it\u2019s positive when the point is cor-\nrectly classi\ufb01ed). Linear loss is the vertical dis-\ntance between an (x, y) point and the graph\n(positive when the point is on the \u2018wrong\u2019 side\nof the graph, i.e., when the graph lies between\nthe point and the zero-altitude plane). Hinge\nloss is linear loss, except no reward is given\nfor any distance beyond the \u2018shelves\u2019 at \u00b11 al-\ntitude. We draw hinge loss in black vertical\nsegments. A hypothesis\u2019s margin is the hori-\nzontal distance the edge of those \u2018shelves\u2019 and\nthe decision boundary.\nBut, continuing the theme of pessimism, we usually feel that a \u201cvery safely\nclassi\ufb01ed\u201d point (very positive leeway) shouldn\u2019t make up for a bunch of \u201cslightly\nmisclassi\ufb01ed\u201d points (slightly negative leeway).\u25e6But total linear loss doesn\u2019t cap-\n\u2190That\nis,\nwe\u2019d\nrather\nhave\nleeways\n+.1, +.1, +.1, +.1\nthan\n+10, \u22121, \u22121, \u22121\non\nfour training examples.\nA very positive\nleeway feels mildly pleasant to us while a very\nnegative one feels urgently alarming.\nFood For Thought: compute and compare the\ntraining accuracies in these two situations. As\nan open-ended followup, suggest reasons why\noptimizing leeways instead of just accuracies\nmight help improve testing accuracy.\nture this asymmetry; to address this, let\u2019s impose a \ufb02oor on linear_loss so that\nit can\u2019t get too negative (i.e., so that positive leeway doesn\u2019t count arbitrarily\nmuch). We get perceptron loss if we set a \ufb02oor of 1; SVM loss (also known as\nhinge loss) if we set a \ufb02oor of 0:\ndef perceptron_loss(x,y,a,b):\nreturn max(1, 1 - leeway_before_mistake(x,y,a,b))\ndef svm_loss(x,y,a,b):\nreturn max(0, 1 - leeway_before_mistake(x,y,a,b))\nProportional weights have the same accuracies but different hinge losses.\nFood For Thought: Can we say the same of perceptron loss?\nFigure 9:\nHinge loss\u2019s optimization land-\nscape re\ufb02ects con\ufb01dence, unlike training ac-\ncuracy\u2019s.\n\u2014 Left rectangular panes.\nAn\nunder-con\ufb01dent and over-con\ufb01dent hypoth-\nesis.\nThese have weights (a/3, b/3) and\n(3a, 3b), where (a, b) = (\u22123.75, 16.25) mini-\nmizes training hinge loss. The glowing colors\u2019\nwidth indicates how rapidly leeway changes\nas we move farther from the boundary.\n\u2014\nRight shaded box. The (a, b) plane, centered\nat the origin and shaded by hinge loss, with\ntraining optimum blue. Indecisive hs (e.g. if\nthreeness\u22480) suffer, since max(0, 1 \u2212\u0001) \u22481,\n(not 0) when \u0001 \u22480. Hinge loss penalizes over-\ncon\ufb01dent mistakes severely (e.g. when y = 1 yet\nthreeness is huge): max(0, 1\u2212\u0001) is unbounded\nin \u0001. If we start at the origin (a, b) = (0, 0) and\nshoot (to less undercon\ufb01dence) toward the op-\ntimal hypothesis, loss will decrease; but once\nwe pass that optimum (to overcon\ufb01dence), loss\nwill (slowly) start increasing again.\n", "2": "Outline\n\u2023 Non-linear classification and regression \n\u2023 Feature maps, their inner products \n\u2023 Kernel functions induced from feature maps \n\u2023 Kernel methods, kernel perceptron \n\u2023 Other non-linear classifiers (e.g., Random Forest)\nLinear classifiers on the real line\nx\n0\nIn feature space\n\u03c61\n\u03c62\nBack to the real line\nx\n0\n2-dim example\nx1\nBy the end of this section, you\u2019ll be able to\n\u2022 explain how regularization, in its incarna\u0002tion as margin-maximization, counters data\nterms to improve generalization\n\u2022 write a regularized ML program (namely,\nan SVM), to classify high-dimensional data\nhow good is a hypothesis? plausibility \u2014 Now to define intrinsic plausiblity, also known\nas a regularizer. We find a hypothesis more plausible when its \u201ctotal amount of\ndependence\u201d on the features is small. So we\u2019ll focus for now on capturing this\nintution: a hypothesis that depends a lot on many features is less plausible.\n\u25e6 We may \u2190There are many other aspects we might de\u0002sign a regularizer to capture, e.g. a domain\u2019s\nsymmetry. The regularizer is in practice a key\npoint where we inject domain knowledge.\nconveniently quantify this as proportional to a sum of squared weights (jargon:\nL2):\u25e6 implausibility of h = (a, b, \u00b7 \u00b7 \u00b7) = \u03bb(a2 + b2 + \u00b7 \u00b7 \u00b7). In code:\n\u2190Food For Thought: When (a, b) represent\nweights for brightness-width digits features,\nhow do hypotheses with small a2 + b2 visu\u0002ally differ from ones with small 6.86a2 + b2 (a\nperfectly fine variant of our \u2018implausibility\u2019)?\n=</latexit>\nPolynomial features\n\u2023 We can add more polynomial terms \n\u2023 Means lots of features in higher dimensions \nNon-lin. classification & regression\n\u2023 Non-linear classification \n\u2023 Non-linear regression\u2028\n\u2028\n\u2028\n\u2028\n\u2028\n\u2028\n\u2028\n\u2028\n\u2028\n\u2028\ne.g., \nf(x; \u2713, \u27130) = \u2713\u00b7 \u03c6(x) + \u27130\nh(x; \u2713, \u27130) = sign\n!\n\u2713\u00b7 \u03c6(x) + \u27130\n\"\n\u03c6(x) = [x, x2]T\nNon-linear regression\n\u22122\n\u22121\n0\n1\n2\n\u22125\n0\n5\nx\ny\n\u22122\n\u22121\n0\n1\n2\n\u22125\n0\n5\nx\ny\n\u22122\n\u22121\n0\n1\n2\n\u22125\n0\n5\nx\ny\n\u22122\n\u22121\n0\n1\n2\n\u22125\n0\n5\nx\ny\nlinear\n3rd order\n5th order\n7th order\nWhy not feature vectors?\n\u2023 By mapping input examples explicitly into feature \nvectors, and performing linear classification or \nregression on top of such feature vectors, we get a lot of \nexpressive power \n\u2023 But the downside is that these vectors can be quite high \ndimensional\nInner products, kernels\n\u2023 Computing the inner product between two feature \nvectors can be cheap even if the vectors are very high \ndimensional\n\u03c6(x) = [x1, x2, x2\n1,\np\n2x1x2, x2\n2]T\n\u03c6(x0) = [x0\n1, x0\n2, x02\n1,\np\n2x0\n1x0\n2, x02\n2]T\nKernels vs features\n\u2023 For some feature maps, we can evaluate the inner \nproducts very efficiently, e.g., \n\u2023 In those cases, it is advantageous to express the linear \nclassifiers (regression methods) in terms of kernels \nrather than explicitly constructing feature vectors\nRecall perceptron\n\u2713= 0\nrun through i = 1, . . . , n\n\u2713 \u2713+ y(i)\u03c6(x(i))\nif y(i) \u2713\u00b7 \u03c6(x(i)) \uf8ff0\nRecall perceptron\n\u2713= 0\nrun through i = 1, . . . , n\n\u2713 \u2713+ y(i)\u03c6(x(i))\nif y(i) \u2713\u00b7 \u03c6(x(i)) \uf8ff0\nFeature engineering, kernels\n\u2023 Composition rules: \n1. K(x, x0) = 1 is a kernel function.\n2. Let f : Rd ! R and K(x, x0) is a kernel. Then so is\n\u02dc\nK(x, x0) = f(x)K(x, x0)f(x0)\n3. If K1(x, x0) and K2(x, x0) are kernels, then\nK(x, x0) = K1(x, x0) + K2(x, x0) is a kernel\n4. If K1(x, x0) and K2(x, x0) are kernels, then\nK(x, x0) = K1(x, x0)K2(x, x0) is a kernel\nRadial basis kernel\n\u2023 The feature vectors can be infinite dimensional\u2026 this \nmeans that they have unlimited expressive power\nK(x, x0) = exp(\u22121\n2kx \u2212x0k2)\nRadial basis kernel\n\u2023 The feature vectors can be infinite dimensional\u2026 this \nmeans that they have unlimited expressive power\nK(x, x0) = exp(\u22121\n2kx \u2212x0k2)\nRadial basis kernel\n\u2023 The feature vectors can be infinite dimensional\u2026 this \nmeans that they have unlimited expressive power\nK(x, x0) = exp(\u22121\n2kx \u2212x0k2)\nRadial basis kernel\n\u2023 The feature vectors can be infinite dimensional\u2026 this \nmeans that they have unlimited expressive power\nK(x, x0) = exp(\u22121\n2kx \u2212x0k2)\nRadial basis kernel\n\u2023 The feature vectors can be infinite dimensional\u2026 this \nmeans that they have unlimited expressive power\nK(x, x0) = exp(\u22121\n2kx \u2212x0k2)\nRadial basis kernel\n\u2023 The feature vectors can be infinite dimensional\u2026 this \nmeans that they have unlimited expressive power\nK(x, x0) = exp(\u22121\n2kx \u2212x0k2)\nOther non-linear classifiers\n\u2023 Random forest is a good default classifier for (almost) \nany setting \n\u2023 Procedure: \n- boostrap sample \n- build a (randomized) decision tree \n- average predictions (ensemble)\nx1\n<latexit sha1_base64=\"GiHX\n6bQYrC8jUzWcW8H28CrnhPA=\">ACJXicbVDLSsNAFJ34rLVq\n0s3wSK4KokPdFlw47KifUBbymRy2w6dTMLMjbaEfIJb/QS/xp0I\nrvwVp20WtvXAwOGce5l7jhcJrtFxvq219Y3Nre3cTn63sLd/UC\nwdNnQYKwZ1FopQtTyqQXAJdeQoBUpoIEnoOmNbqd+8wmU5qF8x\nEkE3YAOJO9zRtFID+Oe2yuWnYozg71K3IyUSYZar2QVOn7I4gAk\nMkG1brtOhN2EKuRMQJrvxBoiykZ0AG1DJQ1Ad5PZral9ahTf7of\nKPIn2TP27kdBA60ngmcmA4lAve1PxP68dY/+m3AZxQiSzT/qx8\nLG0J4Gt32ugKGYGEKZ4uZWmw2pogxNPfmOBtOdHOAw6URUcembd\nGligqULHsIYn7lvbkguKldcpnTn7vc1ipnFdcp+LeX5ar1azJ\nHDkmJ+SMuOSaVMkdqZE6YWRAXsgrebPerQ/r0/qaj65Z2c4RWY\nD18wuL76Us</latexit>\n<latexit sha1_base64=\"GiHX\n6bQYrC8jUzWcW8H28CrnhPA=\">ACJXicbVDLSsNAFJ34rLVq\n0s3wSK4KokPdFlw47KifUBbymRy2w6dTMLMjbaEfIJb/QS/xp0I\nrvwVp20WtvXAwOGce5l7jhcJrtFxvq219Y3Nre3cTn63sLd/UC\nwdNnQYKwZ1FopQtTyqQXAJdeQoBUpoIEnoOmNbqd+8wmU5qF8x\nEkE3YAOJO9zRtFID+Oe2yuWnYozg71K3IyUSYZar2QVOn7I4gAk\nMkG1brtOhN2EKuRMQJrvxBoiykZ0AG1DJQ1Ad5PZral9ahTf7of\nKPIn2TP27kdBA60ngmcmA4lAve1PxP68dY/+m3AZxQiSzT/qx8\nLG0J4Gt32ugKGYGEKZ4uZWmw2pogxNPfmOBtOdHOAw6URUcembd\nGligqULHsIYn7lvbkguKldcpnTn7vc1ipnFdcp+LeX5ar1azJ\nHDkmJ+SMuOSaVMkdqZE6YWRAXsgrebPerQ/r0/qaj65Z2c4RWY\nD18wuL76Us</latexit>\n<latexit sha1_base64=\"GiHX\n6bQYrC8jUzWcW8H28CrnhPA=\">ACJXicbVDLSsNAFJ34rLVq\n0s3wSK4KokPdFlw47KifUBbymRy2w6dTMLMjbaEfIJb/QS/xp0I\nrvwVp20WtvXAwOGce5l7jhcJrtFxvq219Y3Nre3cTn63sLd/UC\nwdNnQYKwZ1FopQtTyqQXAJdeQoBUpoIEnoOmNbqd+8wmU5qF8x\nEkE3YAOJO9zRtFID+Oe2yuWnYozg71K3IyUSYZar2QVOn7I4gAk\nMkG1brtOhN2EKuRMQJrvxBoiykZ0AG1DJQ1Ad5PZral9ahTf7of\nKPIn2TP27kdBA60ngmcmA4lAve1PxP68dY/+m3AZxQiSzT/qx8\nLG0J4Gt32ugKGYGEKZ4uZWmw2pogxNPfmOBtOdHOAw6URUcembd\nGligqULHsIYn7lvbkguKldcpnTn7vc1ipnFdcp+LeX5ar1azJ\nHDkmJ+SMuOSaVMkdqZE6YWRAXsgrebPerQ/r0/qaj65Z2c4RWY\nD18wuL76Us</latexit>\n<latexit sha1_base64=\"GiHX\n6bQYrC8jUzWcW8H28CrnhPA=\">ACJXicbVDLSsNAFJ34rLVq\n0s3wSK4KokPdFlw47KifUBbymRy2w6dTMLMjbaEfIJb/QS/xp0I\nrvwVp20WtvXAwOGce5l7jhcJrtFxvq219Y3Nre3cTn63sLd/UC\nwdNnQYKwZ1FopQtTyqQXAJdeQoBUpoIEnoOmNbqd+8wmU5qF8x\nEkE3YAOJO9zRtFID+Oe2yuWnYozg71K3IyUSYZar2QVOn7I4gAk\nMkG1brtOhN2EKuRMQJrvxBoiykZ0AG1DJQ1Ad5PZral9ahTf7of\nKPIn2TP27kdBA60ngmcmA4lAve1PxP68dY/+m3AZxQiSzT/qx8\nLG0J4Gt32ugKGYGEKZ4uZWmw2pogxNPfmOBtOdHOAw6URUcembd\nGligqULHsIYn7lvbkguKldcpnTn7vc1ipnFdcp+LeX5ar1azJ\nHDkmJ+SMuOSaVMkdqZE6YWRAXsgrebPerQ/r0/qaj65Z2c4RWY\nD18wuL76Us</latexit>\nx2\n<latexit sha1\n_base64=\"myo5jURk4/mF8PHqL\nhGSN4Tw4wc=\">ACJXicbVDL\nSsNAFJ3UV61VW126CRbBVUmqo\nsuCG5cV7QPaUiaT23boZBJmbrQ\nl5BPc6if4Ne5EcOWvOH0sbOuB\ngcM59zL3HC8SXKPjfFuZjc2t7Z\n3sbm4v39wWCgeNXQYKwZ1Fop\nQtTyqQXAJdeQoBUpoIEnoOmNb\nqd+8wmU5qF8xEkE3YAOJO9zRt\nFID+NepVcoOWVnBnuduAtSIgv\nUekUr3/FDFgcgkQmqdt1IuwmV\nCFnAtJcJ9YQUTaiA2gbKmkAup\nvMbk3tM6P4dj9U5km0Z+rfjYQG\nWk8Cz0wGFId61ZuK/3ntGPs3\nYTLKEaQbP5RPxY2hvY0uO1zBQz\nFxBDKFDe32mxIFWVo6sl1NJju\n5ACHSeikvfpEsTEyxd8hDG+\nMx9c0NyUb7iMs2Z/tzVtZJo1J\n2nbJ7f1mqVhdNZskJOSXnxCX\npEruSI3UCSMD8kJeyZv1bn1Yn9\nbXfDRjLXaOyRKsn1+NrqUt</l\natexit>\n<latexit sha1\n_base64=\"myo5jURk4/mF8PHqL\nhGSN4Tw4wc=\">ACJXicbVDL\nSsNAFJ3UV61VW126CRbBVUmqo\nsuCG5cV7QPaUiaT23boZBJmbrQ\nl5BPc6if4Ne5EcOWvOH0sbOuB\ngcM59zL3HC8SXKPjfFuZjc2t7Z\n3sbm4v39wWCgeNXQYKwZ1Fop\nQtTyqQXAJdeQoBUpoIEnoOmNb\nqd+8wmU5qF8xEkE3YAOJO9zRt\nFID+NepVcoOWVnBnuduAtSIgv\nUekUr3/FDFgcgkQmqdt1IuwmV\nCFnAtJcJ9YQUTaiA2gbKmkAup\nvMbk3tM6P4dj9U5km0Z+rfjYQG\nWk8Cz0wGFId61ZuK/3ntGPs3\nYTLKEaQbP5RPxY2hvY0uO1zBQz\nFxBDKFDe32mxIFWVo6sl1NJju\n5ACHSeikvfpEsTEyxd8hDG+\nMx9c0NyUb7iMs2Z/tzVtZJo1J\n2nbJ7f1mqVhdNZskJOSXnxCX\npEruSI3UCSMD8kJeyZv1bn1Yn9\nbXfDRjLXaOyRKsn1+NrqUt</l\natexit>\n<latexit sha1\n_base64=\"myo5jURk4/mF8PHqL\nhGSN4Tw4wc=\">ACJXicbVDL\nSsNAFJ3UV61VW126CRbBVUmqo\nsuCG5cV7QPaUiaT23boZBJmbrQ\nl5BPc6if4Ne5EcOWvOH0sbOuB\ngcM59zL3HC8SXKPjfFuZjc2t7Z\n3sbm4v39wWCgeNXQYKwZ1Fop\nQtTyqQXAJdeQoBUpoIEnoOmNb\nqd+8wmU5qF8xEkE3YAOJO9zRt\nFID+NepVcoOWVnBnuduAtSIgv\nUekUr3/FDFgcgkQmqdt1IuwmV\nCFnAtJcJ9YQUTaiA2gbKmkAup\nvMbk3tM6P4dj9U5km0Z+rfjYQG\nWk8Cz0wGFId61ZuK/3ntGPs3\nYTLKEaQbP5RPxY2hvY0uO1zBQz\nFxBDKFDe32mxIFWVo6sl1NJju\n5ACHSeikvfpEsTEyxd8hDG+\nMx9c0NyUb7iMs2Z/tzVtZJo1J\n2nbJ7f1mqVhdNZskJOSXnxCX\npEruSI3UCSMD8kJeyZv1bn1Yn9\nbXfDRjLXaOyRKsn1+NrqUt</l\natexit>\n<latexit sha1\n_base64=\"myo5jURk4/mF8PHqL\nhGSN4Tw4wc=\">ACJXicbVDL\nSsNAFJ3UV61VW126CRbBVUmqo\nsuCG5cV7QPaUiaT23boZBJmbrQ\nl5BPc6if4Ne5EcOWvOH0sbOuB\ngcM59zL3HC8SXKPjfFuZjc2t7Z\n3sbm4v39wWCgeNXQYKwZ1Fop\nQtTyqQXAJdeQoBUpoIEnoOmNb\nqd+8wmU5qF8xEkE3YAOJO9zRt\nFID+NepVcoOWVnBnuduAtSIgv\nUekUr3/FDFgcgkQmqdt1IuwmV\nCFnAtJcJ9YQUTaiA2gbKmkAup\nvMbk3tM6P4dj9U5km0Z+rfjYQG\nWk8Cz0wGFId61ZuK/3ntGPs3\nYTLKEaQbP5RPxY2hvY0uO1zBQz\nFxBDKFDe32mxIFWVo6sl1NJju\n5ACHSeikvfpEsTEyxd8hDG+\nMx9c0NyUb7iMs2Z/tzVtZJo1J\n2nbJ7f1mqVhdNZskJOSXnxCX\npEruSI3UCSMD8kJeyZv1bn1Yn9\nbXfDRjLXaOyRKsn1+NrqUt</l\natexit>\n+\n--\n-\n+\n+ ++\n+\n---\n-\n-\n- -\n-\n+\n+\n-\n-\nSummary\n\u2023 We can get non-linear classifiers or regression methods \nby simply mapping examples into feature vectors non-\nlinearly, and applying a linear method on the resulting \nvectors \n\u2023 These feature vectors can be high dimensional, however \n\u2023 We can turn the linear methods into kernel methods by \ncasting the computations in terms of inner products \n\u2023 A kernel function is simply an inner product between two \nfeature vectors \n\u2023 Using kernels is advantageous when the inner products \nare faster to evaluate than using explicit vectors (e.g., \nwhen the vectors would be infinite dimensional!)\n", "3": "intrinsic plausibility\nBy the end of this section, you\u2019ll be able to\n\u2022 explain how regularization, in its incarna-\ntion as margin-maximization, counters data\nterms to improve generalization\n\u2022 write a regularized ML program (namely,\nan SVM), to classify high-dimensional data\nhow good is a hypothesis? plausibility \u2014 Now to de\ufb01ne intrinsic plausiblity, also known\nas a regularizer. We \ufb01nd a hypothesis more plausible when its \u201ctotal amount of\ndependence\u201d on the features is small. So we\u2019ll focus for now on capturing this\nintution: a hypothesis that depends a lot on many features is less plausible.\u25e6We may\n\u2190There are many other aspects we might de-\nsign a regularizer to capture, e.g. a domain\u2019s\nsymmetry. The regularizer is in practice a key\npoint where we inject domain knowledge.\nconveniently quantify this as proportional to a sum of squared weights (jargon:\nL2):\u25e6implausibility of h = (a, b, \u00b7 \u00b7 \u00b7 ) = \u03bb(a2 + b2 + \u00b7 \u00b7 \u00b7 ). In code:\n\u2190Food For Thought:\nWhen (a, b) represent\nweights for brightness-width digits features,\nhow do hypotheses with small a2 + b2 visu-\nally differ from ones with small 6.86a2 + b2 (a\nperfectly \ufb01ne variant of our \u2018implausibility\u2019)?\nLAMBDA = 1.\ndef implausibility(a,b):\nreturn LAMBDA * np.sum(np.square([a,b]))\nIntuitively, the constant \u03bb=LAMBDA tells us how much we care about plausibility\nrelative to goodness-of-\ufb01t-to-data.\nHere\u2019s what the formula means. Each of three friends has a theory\u25e6about\n\u2190AJ insists a bird with a wings shorter than\n1ft can\u2019t \ufb02y far, so it\u2019s sure to sing; Conversely,\nbirds with longer wings never sing. Pat checks\nif the bird grows red feathers, eats shrimp, lives\nnear ice, wakes in the night, and has a bill. If\nand only if an even number of these 5 quali-\nties are true, the bird probably sings. Sandy\nsays shorter wings and nocturnality both make\na bird somewhat more likely to sing.\nwhich birds sing. Which theory do we prefer? Well, AJ seems too con\ufb01dent.\nWingspan may matter but probably not so decisively.\nPat avoids black-and-\nwhite claims, but Pat\u2019s predictions depend substantively on many features: \ufb02ip-\nping any one quality \ufb02ips their prediction. This seems implausible. By contrast,\nSandy\u2019s hypothesis doesn\u2019t depend too strongly on too many features. To me, a\nbird non-expert, Sandy\u2019s seems most plausible.\nNow we can de\ufb01ne the overall undesirability of a hypothesis:\u25e6\n\u2190We\u2019ll use SVM loss but feel free to plug in\nother losses to get different learning behaviors!\ndef objective_function(examples,a,b):\ndata_term = np.sum([svm_loss(x,y,a,b) for x,y in examples])\nregularizer = implausibility(a, b)\nreturn data_term + regularizer\nmargins \u2014 To build intuition let\u2019s suppose \u03bb is a tiny positive number. Then minimizing the\nobjective function is the same as minimizing the data term, the total SVM loss:\nour notion of implausibility breaks ties.\nFigure 10: Balancing goodness-of-\ufb01t against\nintrinsic plausibility leads to hypotheses\nwith large margins.\nA hypothesis\u2019s margin\nis its distance to the closest correctly classi-\n\ufb01ed training point(s). Short stems depict these\ndistances for two hypotheses (black, gray). If\nnot for the rightmost orange point, we\u2019d pre-\nfer black over gray since it has larger mar-\ngins. With large \u03bb (i.e., strong regularization),\nwe might prefer black over gray even with\nthat rightmost orange point included, since ex-\npanding the margin is worth the single mis-\nclassi\ufb01cation.\nNow, how does it break ties?\nMomentarily ignore the Figure\u2019s rightmost\norange point and consider the black hypothesis; its predictions depend only\non an input\u2019s \ufb01rst (vertical) coordinate, so it comes from weights of the form\n(a, b) = (a, 0). The (a, 0) pairs differ in SVM loss. If a \u22480, each point has leeway\nclose to 0 and thus SVM loss close to 1; conversely, if a is huge, each point has\nleeway very positive and thus SVM loss equal to the imposed \ufb02oor: 0. So SVM\nloss is 0 as long as a is so big that each leeway to exceed 1.\nImagine sliding a point through the plane. Its leeway is 0 at the black line and\nchanges by a for every unit we slide vertically. So the farther the point is from the\nblack line, the less a must be before leeway exceeds 1 \u2014 and the happier is the\nregularizer, which wants a small. So minimizing SVM loss with an L2 regularizer\nfavors decision boundaries far from even the closest correctly classi\ufb01ed points! The black\nline\u2019s margins exceed the gray\u2019s, so we favor black.\nFor large \u03bb, then this margin-maximization tendency can be so strong that it\noverrides the data term. Thus, even when we bring back the rightmost orange\npoint we ignored, we might prefer the black hypothesis to the gray one.\nNow for some really good intuition-building brain-food!\nFood For Thought: Identify which point on the gray curve to the right corresponds\nto \u03bb = 0. How about \u03bb = \u221e?\nFood For Thought: We have two weight coef\ufb01cients (corresponding to the hor-\nizontal and vertical axes of the Figure). Based on the \ufb01t-to-data term, which\ncoef\ufb01cient is the loss more sensitive to?\nFood For Thought: Observe that the weight-vs-\u03bb trajectory is curved: it doesn\u2019t in-\nterpolate linearly between its \u03bb = 0 and \u03bb = \u221evalues. Which weight (horizontal\nor vertical) gets suppressed \u2018\ufb01rst\u2019 as we increase \u03bb from 0?\nFood For Thought: By thinking about points at which blue and orange contours\nare mutually tangent, sketch the weight-vs-\u03bb trajectory described in the Figure.\nThat is: check that the Figure is right!\nFigure 11: Regularization suppresses differ-\nent features by different amounts. We show\na contour plot of loss terms over 2D weight\nspace: an L2 regularizer and a \ufb01t-to-data term.\nAs we vary \u03bb from 0 (L2 doesn\u2019t matter) to-\nward \u221e(data\ndoesn\u2019t matter), the optimal\nweight changes. We show this weight-vs-\u03bb tra-\njectory in gray. Warning: For the perceptron\nand hinge notions of \ufb01t-to-data, the latter term\nwon\u2019t look so smooth. Still, the moral about\nregularization applies.\n(And future models\nwe\u2019ll discuss (logistic models, least-squares re-\ngression, etc) are smooth.)\noptimization \u2014 Now that we\u2019ve de\ufb01ned our objective function, we want to \ufb01nd a hypothesis\nh = (a, b) that minimizes it. We\u2019ve already discussed how to nudge the weight\nvector to reduce the badness-of-\ufb01t for a datapoint. How do we nudge it to reduce\nthe implausibility? Well, we reduce the \u03bb term simply by moving a, b closer to 0!\nThat is, we combine an update of the form\nwnew = wold \u2212\u03bbwold\nwith the data update.\n( To get this to match our objective exactly, we should actually write 2\u03bb/N\ninstead of \u03bb. The 2 comes from the second power in L2\u2019s de\ufb01nition; the 1/N,\nmore importantly, comes from the fact that we have N data terms but just 1\nplausiblity term. So if we work row-by-row (datapoint-by-datapoint), we ought\nto divvy up the plausibility term into N many terms, each of strength \u03bb/N. At\nthis point, we can just abstract this reasoning away by de\ufb01ning a new constant \u2014\nsay L \u2014 that secretly is 2\u03bb/N. Later, it\u2019ll be good to know where L comes from. )\nWe end up with\nETA = 0.01\nab = initialize()\nfor t in range(10000):\nxfeatures, y = fetch_datapoint_from(training_examples)\nab = ab + ETA * ( - L * ab\n+ y * xfeats * (0 if max(0., y*ab.dot(xfeatures))==0 else 1) )\nThis is the pegasos algorithm we\u2019ll see in the project. Soon we\u2019ll formalize and\ngeneralize this algorithm using calculus.\nFigure 12: With \u03bb = 0.02 the objective visibly\nprefers weights near 0. We develop an algo-\nrithm to take steps in this plane toward the\nminimum, \u2018rolling down\u2019 the hill so to speak.\nFood For Thought: We\u2019ve discussed the L2 regularizer. Also common is the L1\nregularizer: implausibility of h = (a, b, \u00b7 \u00b7 \u00b7 ) = \u03bb(|a| + |b| + \u00b7 \u00b7 \u00b7 ). Hypotheses opti-\nmized with strong L1 regularization will tend to have zero dependence on many\nfeatures. Explain to yourself and then to a friend what the previous sentence\nmeans, why it is true, and how we might exploit it in practice.\n"}